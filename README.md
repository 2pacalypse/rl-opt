# rl-opt

This is the code for the semester project I did at DIAS/EPFL. It is an implementation
of a deep reinforcement learning based query optimizer. The background and the general design
may be found in the report.

## Guides

- queries2pkl.py:
     assumes you have the join-order-benchmark in the current directory.
this file is responsible for generating a list of Query objects for 113 JOB queries.
The output generated by this file is job_queries.pkl

- split_job_qs.py:
     assumes you have the job_queries.pkl in the current directory.
The output is the split 2 files named job_train_qs.pkl and job_test_qs.pkl.

- traininf.py
    usage: traininf.py [-h]
                   eval_queries train_queries exp_output_dir model_output_dir
                   exp_output_freq model_output_freq

    this is the training of the deep network. for an infinite number of episodes, at each episode,
    we train 10 epochs based on train_queries and then we evaluate the performance on eval_queries
    and retrain from the scratch in the next episode with the feedback for the eval_queries.
    the last two numbers are the frequencies to output the experience and the model.
    the experience is nothing but a pickle file containing a list of Query objects.

    if you run it for the first time, it makes sense to start like this
        example run: `python3 traininf.py job_train_qs.pkl job_train_qs.pkl expout modelout 20 10`
    
    once you have the generated experiences under exp_output_dir then you can start with them by
        `python3 traininf.py job_train_qs.pkl exp_output_dir/540.pkl expout modelout 20 10`
